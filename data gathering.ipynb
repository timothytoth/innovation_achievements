{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de19e18",
   "metadata": {},
   "source": [
    "## Proccess for assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3a77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244bf22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be4d7661",
   "metadata": {},
   "source": [
    "## Data Gathering from bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#BigQuery client\n",
    "project_id = 'eco590'\n",
    "credentials = service_account.Credentials.from_service_account_file('C:/Users/Tim/OneDrive/Desktop/eco590-0165d7bd383e.json')\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# 16 month time range starting from 3 months prior of june 22\n",
    "start_date = pd.to_datetime('2022-03-01')  # March 2022\n",
    "end_date = pd.to_datetime('2023-06-30')    # June 2023\n",
    "\n",
    " # Generate a SQL query to select user login, event type, and event date\n",
    "    # from GitHub archive data for specified event types within a date range\n",
    "    # it then filters events to include only PushEvent, PullRequestEvent,\n",
    "    # CreateEvent, ForkEvent, and IssuesEvent\n",
    "def generate_monthly_query(start_suffix, end_suffix):\n",
    "    \"\"\"Generate SQL query to fetch event dates for each event type per user.\"\"\"\n",
    "    return f\"\"\"\n",
    "SELECT\n",
    "    actor.login AS user_login,\n",
    "    type AS event_type,\n",
    "    FORMAT_TIMESTAMP('%Y-%m-%d', created_at) AS event_date\n",
    "FROM\n",
    "    `githubarchive.month.*`\n",
    "WHERE\n",
    "    _TABLE_SUFFIX BETWEEN '{start_suffix}' AND '{end_suffix}'\n",
    "    AND type IN ('PushEvent', 'PullRequestEvent', 'CreateEvent', 'ForkEvent', 'IssuesEvent')\n",
    "\"\"\"\n",
    "\n",
    "# Prepare for iteration\n",
    "date_range = pd.date_range(start_date, end_date, freq='MS') \n",
    "\n",
    "# Iterate through each month in the date range\n",
    "#using tqdm i kept track of progression of retrival process\n",
    "#also after each month processed i saved it to my file path in case of emergecy with outside errros\n",
    "for i, current_date in enumerate(tqdm(date_range, desc=\"Processing\")):\n",
    "    start_suffix = current_date.strftime('%Y%m')\n",
    "    end_suffix = (current_date + pd.offsets.MonthEnd(1)).strftime('%Y%m')\n",
    "    \n",
    "    #Execute query\n",
    "    monthly_query = generate_monthly_query(start_suffix, end_suffix)\n",
    "    df = client.query(monthly_query).to_dataframe()\n",
    "    \n",
    "    # Check for duplicate user_login values in the current month's DataFrame\n",
    "    duplicates = df.duplicated(subset='user_login', keep=False)\n",
    "    print(f\"Number of duplicates in {start_suffix}: {duplicates.sum()}\")\n",
    "    \n",
    "# aggregate the current month's DataFrame\n",
    "#aggregate the dataframe by 'user_login' to compile event data per user\n",
    "#for each user(row) this operation does the following\n",
    "# - 'event_dates': Aggregates all event dates into a list\n",
    "# - 'event_types': Aggregates all event types into a list\n",
    "# - 'event_counts': Counts the total number of events\n",
    "#The result is a dataframe with a row per user, including their aggregated event information\n",
    "\n",
    "    aggregated_df = df.groupby('user_login').agg(\n",
    "        event_dates=pd.NamedAgg(column='event_date', aggfunc=lambda x: list(x)),\n",
    "        event_types=pd.NamedAgg(column='event_type', aggfunc=lambda x: list(x)),\n",
    "        event_counts=pd.NamedAgg(column='event_type', aggfunc='size')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Save the aggregated DataFrame to path\n",
    "    csv_file_path = f'C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_{start_suffix}.csv'\n",
    "    aggregated_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    print(f\"Aggregated data for {start_suffix} saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49269fe",
   "metadata": {},
   "source": [
    "after this process of retrival i successfully had 16 months of data saved in 16 different csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca326c70",
   "metadata": {},
   "source": [
    "now i had to go through the process of cleaning and manipulating the data to my needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# since i have 16 different csv files one for each month. i now want to join the data and combine it into one\n",
    "#csv fiels are all under same folder / name just with the months date at the end of it to differeneitate all of them\n",
    "\n",
    "base_path = \"C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_\"\n",
    "date_range = pd.date_range(start=\"2022-03\", end=\"2023-06\", freq='M')  # Monthly from March 2022 to June 2023\n",
    "\n",
    "file_paths = [f\"{base_path}{date.strftime('%Y%m')}.csv\" for date in date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8787ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users_per_month = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        unique_users = set(df['user_login'].unique()) \n",
    "        unique_users_per_month.append(unique_users)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ee723",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_in_all_months = set.intersection(*unique_users_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d51569",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfs = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    filtered_df = df[df['user_login'].isin(users_in_all_months)]\n",
    "    filtered_dfs.append(filtered_df)\n",
    "\n",
    "# Concatenate all filtered dataframes\n",
    "final_dataset = pd.concat(filtered_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984425d",
   "metadata": {},
   "source": [
    "Here is the csv file that has 16 months of data with no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80276f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280682f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this dataset we can see each row contains the user_login\n",
    "final_dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad1cc8",
   "metadata": {},
   "source": [
    "since there are still duplicates in this dataframe i will use a group dunction to extract adnd aggreagte all duplicate users into one unique row for that given user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d01116",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['event_dates'] = final_dataset['event_dates'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "final_dataset['event_types'] = final_dataset['event_types'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# Perform the aggregation\n",
    "aggregated_data = final_dataset.groupby('user_login').agg({\n",
    "    'event_counts': 'sum',  # Summing the event_counts for each user across all months\n",
    "    'event_dates': lambda dates: list(pd.core.common.flatten(dates)),  # Concatenating all event_dates lists for each user\n",
    "    'event_types': lambda types: list(pd.core.common.flatten(types)),  # Concatenating all event_types lists for each user\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bbc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9598f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8932c4",
   "metadata": {},
   "source": [
    "this dataset is everything that i need but now i am going to create a loop that differentiates the given time periods and collect only within that months period and return the sum of the 5 different events that i gathered fo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e386bf4d",
   "metadata": {},
   "source": [
    "*started many new notebook at this point to get to this point so i started a new one with this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4d283",
   "metadata": {},
   "source": [
    "now this whole process was so sad and brute force that it turned out to be funny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1730749",
   "metadata": {},
   "source": [
    "i couldnt run a loop in all 16 months at once due to memory error and whatever other error arose from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7fbdf",
   "metadata": {},
   "source": [
    "so for this section of code, what it is, is basically a manual for loop that i did where the first code cell here df1, i would change the file path to the aggregated specific month distquined by the date at the end of the file path code.. I would run the whole section to loop through 1 month only and then change the file path to the next month and do the same process 16 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6b7c75",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTim\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mthesis\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbigquerydata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43maggregated_data_202203.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:488\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1047\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1046\u001b[0m     nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m-> 1047\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col_dict:\n\u001b[0;32m   1051\u001b[0m             \u001b[38;5;66;03m# Any column is actually fine:\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:223\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 223\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    225\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:801\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:880\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1073\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1129\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1465\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1421\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1465\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m   1467\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(r\"C:\\Users\\Tim\\OneDrive\\Desktop\\thesis\\bigquerydata\\aggregated_data_202203.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_df is a list of users who appeared throughout all 16 months of data, thjis was my way to reduce the sample size and deal with memory issues and time constraints\n",
    "users_df = pd.read_csv(r'C:\\Users\\Tim\\OneDrive\\Desktop\\thesis\\users_in_all_months.csv')\n",
    "users_in_all_months = users_df['user_login'].tolist()\n",
    "\n",
    "# Convert users_df['user_login'] to a list for help with processing\n",
    "users_in_all_months = users_df['user_login'].tolist()\n",
    "\n",
    "# Filter df1 to only include rows where user_login is in users_in_all_months\n",
    "df1_filtered = df1[df1['user_login'].isin(users_in_all_months)]\n",
    "\n",
    "df1_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def process_events_without_literal_eval(df):\n",
    "    df_copy = df.copy()  # Work on a copy to avoid SettingWithCopyWarning\n",
    "    \n",
    "    for index, row in df_copy.iterrows():\n",
    "        new_dates = []\n",
    "        new_types = []\n",
    "        event_count_decrement = 0\n",
    "\n",
    "        for date, event_type in zip(row['event_dates'], row['event_types']):\n",
    "            # Check if the date is on or after March 22, 2022\n",
    "            if datetime.strptime(date, '%Y-%m-%d') >= datetime(2022, 3, 22):\n",
    "                new_dates.append(date)\n",
    "                new_types.append(event_type)\n",
    "            else:\n",
    "                event_count_decrement += 1\n",
    "\n",
    "        # Update the DataFrame with the modified lists and adjusted counts\n",
    "        df_copy.at[index, 'event_dates'] = new_dates  # No need to convert to string\n",
    "        df_copy.at[index, 'event_types'] = new_types\n",
    "        df_copy.at[index, 'event_counts'] -= event_count_decrement\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "# Apply the function\n",
    "df1_filtered_processed = process_events_without_literal_eval(df1_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce682b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_filtered_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b667ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df1_filtered is created by filtering or selecting from another DataFrame\n",
    "# Make it an explicit copy to avoid SettingWithCopyWarning\n",
    "df1_filtered = df1_filtered_processed.copy()\n",
    "\n",
    "# Specified event types\n",
    "event_types = ['PullRequestEvent', 'PushEvent', 'IssuesEvent', 'CreateEvent', 'ForkEvent']\n",
    "\n",
    "# Use apply with lambda functions for each event type to count occurrences\n",
    "for event in event_types:\n",
    "    df1_filtered[event] = df1_filtered['event_types'].apply(lambda x: x.count(event))\n",
    "\n",
    "# Now df1_filtered will have new columns for each of the event types with their respective counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78db5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79c2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c977d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e740e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac760e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43521228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52e540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c29ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b321ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cef7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c87197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5769f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c943216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad4856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d67b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9d8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29849a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e87a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7af2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62517ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67b0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a492e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bec9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc35aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd22366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1b014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06231d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125fed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bfaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a18486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73655d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab778c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd107e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb0fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a904399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17f7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7b8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77d531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d0836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7eb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47273267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbe776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341a063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6ebc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759d205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d703fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835113e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb50ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b1bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65a8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf1417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98a1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08803fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce2426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce66918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb2999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675523a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8c0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6dafaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c23e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d344e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
