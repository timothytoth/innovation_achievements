{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#BigQuery client\n",
    "project_id = 'eco590'\n",
    "credentials = service_account.Credentials.from_service_account_file('C:/Users/Tim/OneDrive/Desktop/eco590-0165d7bd383e.json')\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# 16 month time range starting from 3 months prior of june 22\n",
    "start_date = pd.to_datetime('2022-03-01')  # March 2022\n",
    "end_date = pd.to_datetime('2023-06-30')    # June 2023\n",
    "\n",
    " # Generate a SQL query to select user login, event type, and event date\n",
    "    # from GitHub archive data for specified event types within a date range\n",
    "    # it then filters events to include only PushEvent, PullRequestEvent,\n",
    "    # CreateEvent, ForkEvent, and IssuesEvent\n",
    "def generate_monthly_query(start_suffix, end_suffix):\n",
    "    \"\"\"Generate SQL query to fetch event dates for each event type per user.\"\"\"\n",
    "    return f\"\"\"\n",
    "SELECT\n",
    "    actor.login AS user_login,\n",
    "    type AS event_type,\n",
    "    FORMAT_TIMESTAMP('%Y-%m-%d', created_at) AS event_date\n",
    "FROM\n",
    "    `githubarchive.month.*`\n",
    "WHERE\n",
    "    _TABLE_SUFFIX BETWEEN '{start_suffix}' AND '{end_suffix}'\n",
    "    AND type IN ('PushEvent', 'PullRequestEvent', 'CreateEvent', 'ForkEvent', 'IssuesEvent')\n",
    "\"\"\"\n",
    "\n",
    "# Prepare for iteration\n",
    "date_range = pd.date_range(start_date, end_date, freq='MS') \n",
    "\n",
    "# Iterate through each month in the date range\n",
    "#using tqdm i kept track of progression of retrival process\n",
    "#also after each month processed i saved it to my file path in case of emergecy with outside errros\n",
    "for i, current_date in enumerate(tqdm(date_range, desc=\"Processing\")):\n",
    "    start_suffix = current_date.strftime('%Y%m')\n",
    "    end_suffix = (current_date + pd.offsets.MonthEnd(1)).strftime('%Y%m')\n",
    "    \n",
    "    #Execute query\n",
    "    monthly_query = generate_monthly_query(start_suffix, end_suffix)\n",
    "    df = client.query(monthly_query).to_dataframe()\n",
    "    \n",
    "    # Check for duplicate user_login values in the current month's DataFrame\n",
    "    duplicates = df.duplicated(subset='user_login', keep=False)\n",
    "    print(f\"Number of duplicates in {start_suffix}: {duplicates.sum()}\")\n",
    "    \n",
    "# aggregate the current month's DataFrame\n",
    "#aggregate the dataframe by 'user_login' to compile event data per user\n",
    "#for each user(row) this operation does the following\n",
    "# - 'event_dates': Aggregates all event dates into a list\n",
    "# - 'event_types': Aggregates all event types into a list\n",
    "# - 'event_counts': Counts the total number of events\n",
    "#The result is a dataframe with a row per user, including their aggregated event information\n",
    "\n",
    "    aggregated_df = df.groupby('user_login').agg(\n",
    "        event_dates=pd.NamedAgg(column='event_date', aggfunc=lambda x: list(x)),\n",
    "        event_types=pd.NamedAgg(column='event_type', aggfunc=lambda x: list(x)),\n",
    "        event_counts=pd.NamedAgg(column='event_type', aggfunc='size')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Save the aggregated DataFrame to path\n",
    "    csv_file_path = f'C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_{start_suffix}.csv'\n",
    "    aggregated_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    print(f\"Aggregated data for {start_suffix} saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# since i have 16 different csv files one for each month. i now want to join the data and combine it into one\n",
    "#csv fiels are all under same folder / name just with the months date at the end of it to differeneitate all of them\n",
    "\n",
    "base_path = \"C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_\"\n",
    "date_range = pd.date_range(start=\"2022-03\", end=\"2023-06\", freq='M')  # Monthly from March 2022 to June 2023\n",
    "\n",
    "file_paths = [f\"{base_path}{date.strftime('%Y%m')}.csv\" for date in date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users_per_month = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        unique_users = set(df['user_login'].unique()) \n",
    "        unique_users_per_month.append(unique_users)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_in_all_months = set.intersection(*unique_users_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14de4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfs = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    filtered_df = df[df['user_login'].isin(users_in_all_months)]\n",
    "    filtered_dfs.append(filtered_df)\n",
    "\n",
    "# Concatenate all of the filtered dataframes\n",
    "final_dataset = pd.concat(filtered_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this dataset we can see each row contains the unique user_login\n",
    "final_dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c787d",
   "metadata": {},
   "source": [
    "since there are still duplicates in this dataframe i will use a group dunction to extract adnd aggreagte all duplicate users into one unique row for that given user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['event_dates'] = final_dataset['event_dates'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "final_dataset['event_types'] = final_dataset['event_types'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# Perform the aggregation\n",
    "aggregated_data = final_dataset.groupby('user_login').agg({\n",
    "    'event_counts': 'sum',  # Summing the event counts for each user across all months\n",
    "    'event_dates': lambda dates: list(pd.core.common.flatten(dates)),  # Concatenating all event_dates lists for each user\n",
    "    'event_types': lambda types: list(pd.core.common.flatten(types)),  # Concatenating all event_types lists for each user\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is the manual loop i ran for all 16 months due to memory error and whatever else error arose from it, so just replaced the date of the file 16 times and ran the rest of code below ill mark where it stopped\n",
    "df1 = pd.read_csv(r\"C:\\Users\\Tim\\OneDrive\\Desktop\\thesis\\bigquerydata\\aggregated_data_202203.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_df is a list of users who appeared throughout all 16 months of data, thjis was my way to reduce the sample size and deal with memory issues and time constraints\n",
    "users_df = pd.read_csv(r'C:\\Users\\Tim\\OneDrive\\Desktop\\thesis\\users_in_all_months.csv')\n",
    "users_in_all_months = users_df['user_login'].tolist()\n",
    "\n",
    "# Filtering df1 to only include rows where user_login is in users_in_all_months\n",
    "df1_filtered = df1[df1['user_login'].isin(users_in_all_months)]\n",
    "\n",
    "df1_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bcda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def process_events_without_literal_eval(df):\n",
    "    df_copy = df.copy()  # have to create a copy to avoid SettingWithCopyWarning error if i did not\n",
    "    \n",
    "    for index, row in df_copy.iterrows():\n",
    "        new_dates = []\n",
    "        new_types = []\n",
    "        event_count_decrement = 0\n",
    "\n",
    "        for date, event_type in zip(row['event_dates'], row['event_types']):\n",
    "            # Check if the date is on or after March 22, 2022\n",
    "            if datetime.strptime(date, '%Y-%m-%d') >= datetime(2022, 3, 22):\n",
    "                new_dates.append(date)\n",
    "                new_types.append(event_type)\n",
    "            else:\n",
    "                event_count_decrement += 1\n",
    "\n",
    "        # Update the DataFrame with the modified lists and adjusted counts\n",
    "        df_copy.at[index, 'event_dates'] = new_dates  # No need to convert to string\n",
    "        df_copy.at[index, 'event_types'] = new_types\n",
    "        df_copy.at[index, 'event_counts'] -= event_count_decrement\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "# Apply the function\n",
    "df1_filtered_processed = process_events_without_literal_eval(df1_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eccf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_filtered = df1_filtered_processed.copy()\n",
    "\n",
    "# Specified event types i have\n",
    "event_types = ['PullRequestEvent', 'PushEvent', 'IssuesEvent', 'CreateEvent', 'ForkEvent']\n",
    "\n",
    "# Use apply with lambda functions for each event type to count occurrences\n",
    "for event in event_types:\n",
    "    df1_filtered[event] = df1_filtered['event_types'].apply(lambda x: x.count(event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b77c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119887f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5cac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f8305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc340e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ff6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff528d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26399f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef384ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb88cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06e9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1cd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f7520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c942f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3170a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ac72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58422c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5398e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
