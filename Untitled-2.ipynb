{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoupGITHUB_TOKEN =",
    "GITHUB_API = 'https://api.github.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching details for users with achievements...\n",
      "[With Achievements] Repo: auto_migrations, Created: 2008-03-16T00:44:24Z, License: MIT License, Stars: 146, Forks: 44, Is Fork: False\n",
      "[With Achievements] Repo: blackjax, Created: 2009-01-21T20:45:27Z, License: No license, Stars: 12, Forks: 9, Is Fork: False\n",
      "[With Achievements] Repo: errcount, Created: 2008-02-15T21:07:30Z, License: No license, Stars: 9, Forks: 11, Is Fork: False\n",
      "[With Achievements] Repo: git-server, Created: 2012-08-03T05:54:17Z, License: No license, Stars: 3, Forks: 2, Is Fork: True\n",
      "[With Achievements] Repo: github-services, Created: 2008-04-28T23:41:21Z, License: No license, Stars: 408, Forks: 55, Is Fork: True\n",
      "[With Achievements] Repo: ThoughtStream, Created: 2010-03-13T22:17:26Z, License: No license, Stars: 7, Forks: 6, Is Fork: False\n",
      "[With Achievements] Repo: vjot, Created: 2008-02-15T20:55:07Z, License: No license, Stars: 9, Forks: 15, Is Fork: False\n",
      "[With Achievements] Repo: zoned, Created: 2008-05-01T06:11:57Z, License: MIT License, Stars: 11, Forks: 5, Is Fork: False\n",
      "[With Achievements] Repo: abbot-from-scratch, Created: 2010-11-28T17:49:44Z, License: No license, Stars: 9, Forks: 5, Is Fork: False\n",
      "[With Achievements] Repo: abbot-ng, Created: 2010-09-15T04:25:23Z, License: No license, Stars: 3, Forks: 2, Is Fork: False\n",
      "[With Achievements] Repo: activerecord-import, Created: 2012-01-03T07:26:10Z, License: No license, Stars: 3, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: active_params, Created: 2010-04-02T07:32:46Z, License: MIT License, Stars: 8, Forks: 1, Is Fork: False\n",
      "[With Achievements] Repo: agendas, Created: 2014-04-05T16:57:08Z, License: No license, Stars: 1, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: alexandria, Created: 2010-03-24T07:07:30Z, License: No license, Stars: 11, Forks: 2, Is Fork: False\n",
      "[With Achievements] Repo: allocation_counter, Created: 2013-11-05T02:18:29Z, License: MIT License, Stars: 8, Forks: 2, Is Fork: False\n",
      "[With Achievements] Repo: argon, Created: 2018-06-17T06:33:41Z, License: No license, Stars: 13, Forks: 1, Is Fork: False\n",
      "[With Achievements] Repo: argon-example, Created: 2018-06-28T06:48:38Z, License: No license, Stars: 2, Forks: 0, Is Fork: False\n",
      "[With Achievements] Repo: artifice, Created: 2010-03-25T01:36:42Z, License: MIT License, Stars: 214, Forks: 15, Is Fork: False\n",
      "[With Achievements] Repo: asdf, Created: 2010-08-17T00:42:23Z, License: No license, Stars: 36, Forks: 5, Is Fork: False\n",
      "[With Achievements] Repo: at-media, Created: 2008-09-16T10:56:47Z, License: No license, Stars: 4, Forks: 2, Is Fork: False\n",
      "[With Achievements] Repo: atom-pain-split, Created: 2015-09-28T03:42:07Z, License: GNU Affero General Public License v3.0, Stars: 1, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: audiobookshelf, Created: 2023-07-18T22:54:23Z, License: GNU General Public License v3.0, Stars: 0, Forks: 0, Is Fork: True\n",
      "[With Achievements] Repo: bench-backburner, Created: 2017-03-31T19:08:00Z, License: No license, Stars: 1, Forks: 0, Is Fork: True\n",
      "[With Achievements] Repo: benchwarmer, Created: 2008-05-03T21:22:10Z, License: MIT License, Stars: 32, Forks: 5, Is Fork: False\n",
      "[With Achievements] Repo: blue-ridge, Created: 2009-05-03T21:39:35Z, License: MIT License, Stars: 22, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: bootstrap-nitrous, Created: 2013-10-13T07:56:38Z, License: MIT License, Stars: 3, Forks: 1, Is Fork: False\n",
      "[With Achievements] Repo: broccoli-concat, Created: 2016-01-06T20:01:05Z, License: MIT License, Stars: 1, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: broccoli-typescript-compiler, Created: 2019-07-10T23:16:12Z, License: MIT License, Stars: 1, Forks: 0, Is Fork: True\n",
      "[With Achievements] Repo: bundler, Created: 2009-07-06T22:08:44Z, License: No license, Stars: 410, Forks: 30, Is Fork: False\n",
      "[With Achievements] Repo: cafe, Created: 2011-06-17T16:38:59Z, License: No license, Stars: 4, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: cargo, Created: 2014-06-26T22:18:01Z, License: Apache License 2.0, Stars: 2, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: cargo-docserve, Created: 2021-10-26T14:15:19Z, License: No license, Stars: 0, Forks: 0, Is Fork: True\n",
      "[With Achievements] Repo: cargo-website, Created: 2014-06-23T03:34:27Z, License: No license, Stars: 8, Forks: 18, Is Fork: False\n",
      "[With Achievements] Repo: chacha, Created: 2021-02-25T23:58:46Z, License: No license, Stars: 2, Forks: 0, Is Fork: False\n",
      "[With Achievements] Repo: chainable_compare, Created: 2010-07-14T00:21:55Z, License: No license, Stars: 12, Forks: 2, Is Fork: False\n",
      "[With Achievements] Repo: changesets, Created: 2022-06-21T18:55:28Z, License: MIT License, Stars: 0, Forks: 0, Is Fork: True\n",
      "[With Achievements] Repo: codespaces-setup, Created: 2022-05-26T21:02:02Z, License: MIT License, Stars: 0, Forks: 0, Is Fork: False\n",
      "[With Achievements] Repo: codespan, Created: 2018-07-08T17:51:21Z, License: Apache License 2.0, Stars: 2, Forks: 0, Is Fork: True\n",
      "[With Achievements] Repo: acl_system2, Created: 2008-03-25T19:00:13Z, License: MIT License, Stars: 56, Forks: 15, Is Fork: False\n",
      "[With Achievements] Repo: bmhsearch, Created: 2008-01-28T02:22:50Z, License: Other, Stars: 13, Forks: 7, Is Fork: False\n",
      "[With Achievements] Repo: chef-101, Created: 2009-01-31T17:28:08Z, License: No license, Stars: 62, Forks: 8, Is Fork: False\n",
      "[With Achievements] Repo: chef-deploy, Created: 2009-03-24T21:22:28Z, License: Apache License 2.0, Stars: 291, Forks: 31, Is Fork: False\n",
      "[With Achievements] Repo: ey-lessql, Created: 2009-08-14T03:17:52Z, License: No license, Stars: 39, Forks: 10, Is Fork: False\n",
      "[With Achievements] Repo: ez-scheme, Created: 2010-11-23T01:55:24Z, License: No license, Stars: 27, Forks: 4, Is Fork: False\n",
      "[With Achievements] Repo: ez-where, Created: 2008-06-20T20:14:55Z, License: No license, Stars: 33, Forks: 3, Is Fork: False\n",
      "[With Achievements] Repo: heist, Created: 2010-11-22T21:57:27Z, License: No license, Stars: 1, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: LocheGSplicer, Created: 2012-07-29T23:27:24Z, License: GNU Lesser General Public License v3.0, Stars: 6, Forks: 3, Is Fork: False\n",
      "[With Achievements] Repo: lua-nginx-module, Created: 2011-02-14T03:51:50Z, License: No license, Stars: 2, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: merbivore, Created: 2008-05-04T20:37:24Z, License: No license, Stars: 4, Forks: 3, Is Fork: False\n",
      "[With Achievements] Repo: nanite, Created: 2008-03-31T08:32:08Z, License: Apache License 2.0, Stars: 737, Forks: 71, Is Fork: False\n",
      "[With Achievements] Repo: nats, Created: 2010-11-21T10:29:27Z, License: MIT License, Stars: 1, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: nginx-ey-balancer, Created: 2009-01-22T19:53:59Z, License: No license, Stars: 39, Forks: 19, Is Fork: False\n",
      "[With Achievements] Repo: ohai, Created: 2009-01-15T21:58:21Z, License: Apache License 2.0, Stars: 5, Forks: 0, Is Fork: True\n",
      "[With Achievements] Repo: redactor, Created: 2010-01-02T05:49:55Z, License: MIT License, Stars: 66, Forks: 1, Is Fork: False\n",
      "[With Achievements] Repo: redis, Created: 2009-03-23T01:54:07Z, License: BSD 3-Clause \"New\" or \"Revised\" License, Stars: 8, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: redis-rb, Created: 2012-03-14T12:48:05Z, License: MIT License, Stars: 29, Forks: 10, Is Fork: True\n",
      "[With Achievements] Repo: super-nginx, Created: 2011-02-11T23:33:28Z, License: No license, Stars: 146, Forks: 10, Is Fork: False\n",
      "[With Achievements] Repo: tlabs-mendelmax, Created: 2012-07-11T02:04:19Z, License: No license, Stars: 8, Forks: 3, Is Fork: False\n",
      "[With Achievements] Repo: vcap-tests, Created: 2011-05-12T20:55:30Z, License: Apache License 2.0, Stars: 1, Forks: 1, Is Fork: True\n",
      "[With Achievements] Repo: vmc, Created: 2011-05-13T08:12:48Z, License: Other, Stars: 3, Forks: 2, Is Fork: True\n",
      "\n",
      "Fetching details for users without achievements...\n",
      "[No Achievements] Repo: 30daysoflaptops.github.io, Created: 2014-11-20T06:42:06Z, License: No license, Stars: 8, Forks: 4, Is Fork: False\n",
      "[No Achievements] Repo: asteroids, Created: 2014-03-03T07:40:00Z, License: Other, Stars: 94, Forks: 14, Is Fork: False\n",
      "[No Achievements] Repo: benbalter.github.com, Created: 2015-01-27T23:54:05Z, License: No license, Stars: 6, Forks: 6, Is Fork: True\n",
      "[No Achievements] Repo: bert, Created: 2009-10-08T06:06:25Z, License: MIT License, Stars: 205, Forks: 72, Is Fork: False\n",
      "[No Achievements] Repo: bert.erl, Created: 2009-12-21T02:10:34Z, License: MIT License, Stars: 98, Forks: 50, Is Fork: False\n",
      "[No Achievements] Repo: bertrpc, Created: 2009-05-19T02:44:01Z, License: MIT License, Stars: 162, Forks: 32, Is Fork: False\n",
      "[No Achievements] Repo: bower, Created: 2012-11-13T02:32:34Z, License: MIT License, Stars: 6, Forks: 3, Is Fork: True\n",
      "[No Achievements] Repo: chronic, Created: 2008-01-29T06:48:49Z, License: MIT License, Stars: 3222, Forks: 461, Is Fork: False\n",
      "[No Achievements] Repo: clippy, Created: 2009-02-13T18:59:10Z, License: MIT License, Stars: 940, Forks: 171, Is Fork: False\n",
      "[No Achievements] Repo: conceptual_algorithms, Created: 2008-09-20T04:54:25Z, License: No license, Stars: 5, Forks: 2, Is Fork: False\n",
      "[No Achievements] Repo: cubesixel, Created: 2009-03-05T00:00:25Z, License: MIT License, Stars: 26, Forks: 4, Is Fork: False\n",
      "[No Achievements] Repo: docz-website, Created: 2018-11-10T06:07:54Z, License: No license, Stars: 1, Forks: 0, Is Fork: True\n",
      "[No Achievements] Repo: egitd, Created: 2008-05-24T20:36:13Z, License: MIT License, Stars: 115, Forks: 19, Is Fork: False\n",
      "[No Achievements] Repo: endo, Created: 2009-02-23T22:48:51Z, License: No license, Stars: 4, Forks: 2, Is Fork: False\n",
      "[No Achievements] Repo: erlang_pipe, Created: 2008-07-01T16:40:14Z, License: No license, Stars: 21, Forks: 3, Is Fork: False\n",
      "[No Achievements] Repo: erlectricity, Created: 2008-03-08T01:32:13Z, License: MIT License, Stars: 346, Forks: 58, Is Fork: False\n",
      "[No Achievements] Repo: erlectricity-presentation, Created: 2009-04-30T07:05:01Z, License: No license, Stars: 5, Forks: 4, Is Fork: False\n",
      "[No Achievements] Repo: erlenmeyer, Created: 2008-02-28T03:17:49Z, License: No license, Stars: 9, Forks: 0, Is Fork: True\n",
      "[No Achievements] Repo: ernie, Created: 2009-05-19T01:51:40Z, License: MIT License, Stars: 459, Forks: 59, Is Fork: False\n",
      "[No Achievements] Repo: eventmachine, Created: 2009-10-04T20:24:34Z, License: No license, Stars: 12, Forks: 4, Is Fork: True\n",
      "[No Achievements] Repo: fakegem, Created: 2009-02-14T01:48:53Z, License: MIT License, Stars: 4, Forks: 1, Is Fork: False\n",
      "[No Achievements] Repo: fixture-scenarios, Created: 2008-02-23T04:25:57Z, License: MIT License, Stars: 18, Forks: 16, Is Fork: False\n",
      "[No Achievements] Repo: git, Created: 2008-10-31T22:57:51Z, License: Other, Stars: 12, Forks: 5, Is Fork: True\n",
      "[No Achievements] Repo: git-bzr, Created: 2008-09-16T20:04:23Z, License: No license, Stars: 8, Forks: 2, Is Fork: True\n",
      "[No Achievements] Repo: github-flavored-markdown, Created: 2009-04-10T21:57:08Z, License: No license, Stars: 182, Forks: 89, Is Fork: False\n",
      "[No Achievements] Repo: github-gem, Created: 2008-09-30T22:29:14Z, License: MIT License, Stars: 6, Forks: 0, Is Fork: True\n",
      "[No Achievements] Repo: glowstick, Created: 2008-01-17T00:40:56Z, License: No license, Stars: 36, Forks: 18, Is Fork: False\n",
      "[No Achievements] Repo: god, Created: 2008-01-13T05:16:23Z, License: MIT License, Stars: 2208, Forks: 593, Is Fork: False\n",
      "[No Achievements] Repo: gollum-demo, Created: 2010-08-04T23:49:22Z, License: No license, Stars: 75, Forks: 207, Is Fork: False\n",
      "[No Achievements] Repo: grit, Created: 2007-10-29T14:37:16Z, License: MIT License, Stars: 1965, Forks: 540, Is Fork: False\n",
      "[No Achievements] Repo: ace, Created: 2011-06-07T18:41:40Z, License: Other, Stars: 17, Forks: 7, Is Fork: True\n",
      "[No Achievements] Repo: acts_as_textiled, Created: 2008-03-12T06:20:18Z, License: MIT License, Stars: 115, Forks: 35, Is Fork: False\n",
      "[No Achievements] Repo: ambition, Created: 2008-01-14T06:28:56Z, License: MIT License, Stars: 166, Forks: 28, Is Fork: False\n",
      "[No Achievements] Repo: ambitious_activeldap, Created: 2008-01-30T19:20:08Z, License: MIT License, Stars: 13, Forks: 8, Is Fork: False\n",
      "[No Achievements] Repo: ambitious_activerecord, Created: 2008-04-26T09:10:20Z, License: MIT License, Stars: 14, Forks: 4, Is Fork: False\n",
      "[No Achievements] Repo: barefootexamples, Created: 2008-03-17T06:29:50Z, License: No license, Stars: 7, Forks: 5, Is Fork: False\n",
      "[No Achievements] Repo: body_matcher, Created: 2008-05-11T04:54:44Z, License: MIT License, Stars: 10, Forks: 2, Is Fork: True\n",
      "[No Achievements] Repo: burn, Created: 2009-08-26T01:31:54Z, License: No license, Stars: 7, Forks: 3, Is Fork: False\n",
      "[No Achievements] Repo: cache_fu, Created: 2008-01-23T00:28:10Z, License: MIT License, Stars: 260, Forks: 83, Is Fork: False\n",
      "[No Achievements] Repo: cheat, Created: 2008-03-12T06:09:09Z, License: MIT License, Stars: 240, Forks: 45, Is Fork: False\n",
      "[No Achievements] Repo: cheat.el, Created: 2008-08-23T06:01:37Z, License: No license, Stars: 15, Forks: 4, Is Fork: False\n",
      "[No Achievements] Repo: choice, Created: 2008-04-25T18:30:30Z, License: MIT License, Stars: 177, Forks: 21, Is Fork: False\n",
      "[No Achievements] Repo: cijoe, Created: 2009-08-06T00:20:39Z, License: MIT License, Stars: 1048, Forks: 136, Is Fork: False\n",
      "[No Achievements] Repo: coffee-mode, Created: 2010-03-07T08:30:40Z, License: No license, Stars: 573, Forks: 154, Is Fork: False\n",
      "[No Achievements] Repo: colored, Created: 2009-11-28T06:16:20Z, License: MIT License, Stars: 270, Forks: 43, Is Fork: False\n",
      "[No Achievements] Repo: currency_converter, Created: 2008-04-24T09:34:31Z, License: No license, Stars: 9, Forks: 4, Is Fork: False\n",
      "[No Achievements] Repo: d3, Created: 2014-04-08T18:45:26Z, License: Other, Stars: 5, Forks: 1, Is Fork: True\n",
      "[No Achievements] Repo: defunkt.github.com, Created: 2008-12-17T07:53:14Z, License: No license, Stars: 77, Forks: 57, Is Fork: False\n",
      "[No Achievements] Repo: djangode, Created: 2010-04-25T16:41:30Z, License: BSD 2-Clause \"Simplified\" License, Stars: 10, Forks: 3, Is Fork: True\n",
      "[No Achievements] Repo: dodgeball.github.com, Created: 2011-09-24T03:01:09Z, License: No license, Stars: 12, Forks: 5, Is Fork: False\n",
      "[No Achievements] Repo: dotenv, Created: 2012-07-24T21:43:19Z, License: MIT License, Stars: 11, Forks: 2, Is Fork: True\n",
      "[No Achievements] Repo: dotjs, Created: 2011-02-07T07:01:33Z, License: MIT License, Stars: 3159, Forks: 366, Is Fork: False\n",
      "[No Achievements] Repo: electron-wordwrap, Created: 2008-10-29T20:03:17Z, License: No license, Stars: 6, Forks: 4, Is Fork: False\n",
      "[No Achievements] Repo: emacs, Created: 2008-08-19T10:50:19Z, License: No license, Stars: 187, Forks: 45, Is Fork: False\n",
      "[No Achievements] Repo: email_reply_parser, Created: 2011-12-16T23:13:05Z, License: MIT License, Stars: 9, Forks: 4, Is Fork: True\n",
      "[No Achievements] Repo: evilbot, Created: 2010-12-14T08:09:11Z, License: MIT License, Stars: 50, Forks: 16, Is Fork: False\n",
      "[No Achievements] Repo: exception_logger, Created: 2008-01-14T03:32:19Z, License: No license, Stars: 244, Forks: 98, Is Fork: False\n",
      "[No Achievements] Repo: facebox, Created: 2008-02-11T22:49:27Z, License: MIT License, Stars: 1927, Forks: 406, Is Fork: False\n",
      "[No Achievements] Repo: faceup, Created: 2012-07-28T02:11:56Z, License: MIT License, Stars: 11, Forks: 5, Is Fork: True\n",
      "[No Achievements] Repo: fixture_scenarios_builder, Created: 2008-03-12T06:24:02Z, License: MIT License, Stars: 15, Forks: 6, Is Fork: False\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Your GitHub token for authentication\n",
    "headers = {'Authorization': 'hp_2N58AVd6T7dgNK9aUJttVl8xnhVx8S09rKrx'}\n",
    "\n",
    "# Users with achievements\n",
    "users_with_achievements = ['pjhyett', 'wycats', 'ezmobius']\n",
    "# Users without achievements\n",
    "users_no_achievement = ['mojombo', 'defunkt']\n",
    "\n",
    "# Function to fetch and print repo details\n",
    "def fetch_repo_details(users, achievement_status):\n",
    "    for user in users:\n",
    "        repos_response = requests.get(f'https://api.github.com/users/{user}/repos', headers=headers)\n",
    "        if repos_response.status_code == 200:\n",
    "            repos = repos_response.json()\n",
    "            for repo in repos:\n",
    "                name = repo['name']\n",
    "                creation_date = repo['created_at']\n",
    "                license = repo['license']['name'] if repo['license'] else 'No license'\n",
    "                stargazers = repo['stargazers_count']\n",
    "                forks = repo['forks_count']\n",
    "                is_fork = repo['fork']\n",
    "                print(f'[{achievement_status}] Repo: {name}, Created: {creation_date}, License: {license}, Stars: {stargazers}, Forks: {forks}, Is Fork: {is_fork}')\n",
    "        else:\n",
    "            print(f'Failed to fetch repositories for user: {user}')\n",
    "\n",
    "# Fetch and print details for each group\n",
    "print(\"Fetching details for users with achievements...\")\n",
    "fetch_repo_details(users_with_achievements, \"With Achievements\")\n",
    "\n",
    "print(\"\\nFetching details for users without achievements...\")\n",
    "fetch_repo_details(users_no_achievement, \"No Achievements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 16/16 [16:03:27<00:00, 3612.94s/it]  \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 31.6 GiB for an array with shape (3, 1415902020) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     monthly_results\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Combine all monthly DataFrames into a single DataFrame\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonthly_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Initialize a master dictionary to hold user events\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\reshape\\concat.py:307\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03malong the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03mValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    295\u001b[0m     objs,\n\u001b[0;32m    296\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    305\u001b[0m )\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\reshape\\concat.py:532\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    530\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 532\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[0;32m    536\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\concat.py:216\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    210\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m#  than concat_compat\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 31.6 GiB for an array with shape (3, 1415902020) and data type object"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Set up BigQuery client\n",
    "project_id = 'eco590'\n",
    "credentials = service_account.Credentials.from_service_account_file('C:/Users/Tim/OneDrive/Desktop/eco590-0165d7bd383e.json')\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Define the time range for data analysis\n",
    "start_date = pd.to_datetime('2022-03-01')  # March 2022\n",
    "end_date = pd.to_datetime('2023-06-30')    # June 2023\n",
    "\n",
    "def generate_monthly_query(start_suffix, end_suffix):\n",
    "    \"\"\"Generate SQL query to fetch event dates for each event type per user.\"\"\"\n",
    "    return f\"\"\"\n",
    "SELECT\n",
    "    actor.login AS user_login,\n",
    "    type AS event_type,\n",
    "    FORMAT_TIMESTAMP('%Y-%m-%d', created_at) AS event_date\n",
    "FROM\n",
    "    `githubarchive.month.*`\n",
    "WHERE\n",
    "    _TABLE_SUFFIX BETWEEN '{start_suffix}' AND '{end_suffix}'\n",
    "    AND type IN ('PushEvent', 'PullRequestEvent', 'CreateEvent', 'ForkEvent', 'IssuesEvent')\n",
    "\"\"\"\n",
    "\n",
    "# Prepare for iteration\n",
    "total_months = (end_date.year - start_date.year) * 12 + end_date.month - start_date.month + 1\n",
    "monthly_results = []\n",
    "date_range = pd.date_range(start_date, end_date, freq='MS')  # 'MS' is month start frequency\n",
    "\n",
    "# Iterate through each month in the date range\n",
    "for current_date in tqdm(date_range, total=total_months, desc=\"Processing\"):\n",
    "    start_suffix = current_date.strftime('%Y%m')\n",
    "    end_suffix = (current_date + pd.offsets.MonthEnd(1)).strftime('%Y%m')\n",
    "    \n",
    "    monthly_query = generate_monthly_query(start_suffix, end_suffix)\n",
    "    df = client.query(monthly_query).to_dataframe()\n",
    "    monthly_results.append(df)\n",
    "\n",
    "# Combine all monthly DataFrames into a single DataFrame\n",
    "final_df = pd.concat(monthly_results, ignore_index=True)\n",
    "final_df['event_date'] = pd.to_datetime(final_df['event_date'])\n",
    "\n",
    "# Initialize a master dictionary to hold user events\n",
    "user_events_dates = {}\n",
    "\n",
    "# Populate the dictionary with lists of dates for each user and event type\n",
    "for _, row in final_df.iterrows():\n",
    "    user_login = row['user_login']\n",
    "    event_type = row['event_type'].lower() + '_dates'\n",
    "    event_date = row['event_date']\n",
    "    \n",
    "    if user_login not in user_events_dates:\n",
    "        user_events_dates[user_login] = {'repos_creation_dates': [], 'commits_dates': [], 'issues_dates': [], 'forks_dates': []}\n",
    "    user_events_dates[user_login][event_type].append(event_date)\n",
    "\n",
    "# Convert the nested dictionaries into a DataFrame\n",
    "users_df = pd.DataFrame.from_dict(user_events_dates, orient='index').reset_index()\n",
    "users_df.rename(columns={'index': 'user_login'}, inplace=True)\n",
    "\n",
    "csv_file_path = 'C:/Users/Tim/OneDrive/Desktop/thesis/code.csv'  # Specify your path and file name\n",
    "users_df.to_csv(csv_file_path, index=False)  # Setting index=False to avoid saving the DataFrame index\n",
    "\n",
    "print(f\"Data saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in 202203: 80334006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▋         | 1/16 [57:54<14:28:32, 3474.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202203 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202203.csv\n",
      "Number of duplicates in 202204: 78513577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▎        | 2/16 [1:52:42<13:05:11, 3365.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202204 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202204.csv\n",
      "Number of duplicates in 202205: 76485877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 3/16 [2:47:25<12:00:57, 3327.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202205 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202205.csv\n",
      "Number of duplicates in 202206: 76336933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 4/16 [3:41:23<10:58:24, 3292.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202206 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202206.csv\n",
      "Number of duplicates in 202207: 80730558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███▏      | 5/16 [4:39:02<10:14:37, 3352.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202207 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202207.csv\n",
      "Number of duplicates in 202208: 85610419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 6/16 [5:40:50<9:38:51, 3473.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202208 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202208.csv\n",
      "Number of duplicates in 202209: 90579378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▍     | 7/16 [6:45:05<8:59:42, 3598.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202209 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202209.csv\n",
      "Number of duplicates in 202210: 101387710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 8/16 [8:04:16<8:48:39, 3964.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202210 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202210.csv\n",
      "Number of duplicates in 202211: 91958769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▋    | 9/16 [9:07:20<7:35:58, 3908.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202211 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202211.csv\n",
      "Number of duplicates in 202212: 97315079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▎   | 10/16 [10:12:35<6:31:03, 3910.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202212 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202212.csv\n",
      "Number of duplicates in 202301: 96721720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▉   | 11/16 [11:23:52<5:35:13, 4022.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202301 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202301.csv\n",
      "Number of duplicates in 202302: 86507732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 12/16 [12:23:14<4:18:50, 3882.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202302 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202302.csv\n",
      "Number of duplicates in 202303: 95678393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████▏ | 13/16 [13:36:49<3:22:11, 4043.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202303 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202303.csv\n",
      "Number of duplicates in 202304: 88646492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 14/16 [14:39:50<2:12:08, 3964.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202304 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202304.csv\n",
      "Number of duplicates in 202305: 89451108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▍| 15/16 [15:45:22<1:05:54, 3954.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202305 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202305.csv\n",
      "Number of duplicates in 202306: 85699125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 16/16 [16:47:04<00:00, 3776.54s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data for 202306 saved to C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_202306.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Set up BigQuery client\n",
    "project_id = 'eco590'\n",
    "credentials = service_account.Credentials.from_service_account_file('C:/Users/Tim/OneDrive/Desktop/eco590-0165d7bd383e.json')\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Define the time range for data analysis\n",
    "start_date = pd.to_datetime('2022-03-01')  # March 2022\n",
    "end_date = pd.to_datetime('2023-06-30')    # June 2023\n",
    "\n",
    "def generate_monthly_query(start_suffix, end_suffix):\n",
    "    \"\"\"Generate SQL query to fetch event dates for each event type per user.\"\"\"\n",
    "    return f\"\"\"\n",
    "SELECT\n",
    "    actor.login AS user_login,\n",
    "    type AS event_type,\n",
    "    FORMAT_TIMESTAMP('%Y-%m-%d', created_at) AS event_date\n",
    "FROM\n",
    "    `githubarchive.month.*`\n",
    "WHERE\n",
    "    _TABLE_SUFFIX BETWEEN '{start_suffix}' AND '{end_suffix}'\n",
    "    AND type IN ('PushEvent', 'PullRequestEvent', 'CreateEvent', 'ForkEvent', 'IssuesEvent')\n",
    "\"\"\"\n",
    "\n",
    "# Prepare for iteration\n",
    "date_range = pd.date_range(start_date, end_date, freq='MS')  # 'MS' is month start frequency\n",
    "\n",
    "# Iterate through each month in the date range\n",
    "for i, current_date in enumerate(tqdm(date_range, desc=\"Processing\")):\n",
    "    start_suffix = current_date.strftime('%Y%m')\n",
    "    end_suffix = (current_date + pd.offsets.MonthEnd(1)).strftime('%Y%m')\n",
    "    \n",
    "    # Execute query\n",
    "    monthly_query = generate_monthly_query(start_suffix, end_suffix)\n",
    "    df = client.query(monthly_query).to_dataframe()\n",
    "    \n",
    "    # Check for duplicate user_login values in the current month's DataFrame\n",
    "    duplicates = df.duplicated(subset='user_login', keep=False)\n",
    "    print(f\"Number of duplicates in {start_suffix}: {duplicates.sum()}\")\n",
    "    \n",
    "    # Aggregate the current month's DataFrame\n",
    "    aggregated_df = df.groupby('user_login').agg(\n",
    "        event_dates=pd.NamedAgg(column='event_date', aggfunc=lambda x: list(x)),\n",
    "        event_types=pd.NamedAgg(column='event_type', aggfunc=lambda x: list(x)),\n",
    "        event_counts=pd.NamedAgg(column='event_type', aggfunc='size')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Save the aggregated DataFrame to a CSV file\n",
    "    csv_file_path = f'C:/Users/Tim/OneDrive/Desktop/thesis/aggregated_data_{start_suffix}.csv'\n",
    "    aggregated_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    print(f\"Aggregated data for {start_suffix} saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m      2\u001b[0m unique_event_types \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(unique_event_types)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "unique_event_types = df['event_type'].unique()\n",
    "print(unique_event_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                    user_login        event_type  event_date\n",
       " 0                      Qyanjia         ForkEvent  2022-03-06\n",
       " 1                      JLHwung         ForkEvent  2022-03-05\n",
       " 2                   paulovigne         ForkEvent  2022-03-05\n",
       " 3                 PENGZhaoqing         ForkEvent  2022-03-06\n",
       " 4                         f7cw         ForkEvent  2022-03-06\n",
       " ...                        ...               ...         ...\n",
       " 81361824                 GianW  PullRequestEvent  2022-03-06\n",
       " 81361825         renovate[bot]  PullRequestEvent  2022-03-05\n",
       " 81361826             pull[bot]  PullRequestEvent  2022-03-05\n",
       " 81361827  GorleRajeshBharadwaj  PullRequestEvent  2022-03-05\n",
       " 81361828               lyozamp  PullRequestEvent  2022-03-06\n",
       " \n",
       " [81361829 rows x 3 columns],\n",
       "                  user_login        event_type  event_date\n",
       " 0              alicangunduz         ForkEvent  2022-04-04\n",
       " 1             garimazthakur         ForkEvent  2022-04-05\n",
       " 2               kieronellis         ForkEvent  2022-04-05\n",
       " 3         dtt-brunomisantos         ForkEvent  2022-04-05\n",
       " 4                 MOJarrett         ForkEvent  2022-04-05\n",
       " ...                     ...               ...         ...\n",
       " 79453856           SadiqZak  PullRequestEvent  2022-04-05\n",
       " 79453857      SebastianPfeu  PullRequestEvent  2022-04-05\n",
       " 79453858       RamizAshurov  PullRequestEvent  2022-04-05\n",
       " 79453859              Appw0  PullRequestEvent  2022-04-05\n",
       " 79453860    direwolf-github  PullRequestEvent  2022-04-05\n",
       " \n",
       " [79453861 rows x 3 columns],\n",
       "                user_login        event_type  event_date\n",
       " 0                xmcaohhu         ForkEvent  2022-05-09\n",
       " 1            chandlerroth         ForkEvent  2022-05-06\n",
       " 2                baile633         ForkEvent  2022-05-23\n",
       " 3              simbolmina         ForkEvent  2022-05-16\n",
       " 4                Korenely         ForkEvent  2022-05-23\n",
       " ...                   ...               ...         ...\n",
       " 77404708    graeme-winter  PullRequestEvent  2022-05-22\n",
       " 77404709        pull[bot]  PullRequestEvent  2022-05-13\n",
       " 77404710         fourside  PullRequestEvent  2022-05-13\n",
       " 77404711  dependabot[bot]  PullRequestEvent  2022-05-13\n",
       " 77404712      Himansh2001  PullRequestEvent  2022-05-13\n",
       " \n",
       " [77404713 rows x 3 columns],\n",
       "                   user_login        event_type  event_date\n",
       " 0                  yixingjia         ForkEvent  2022-06-08\n",
       " 1                    WayChan         ForkEvent  2022-06-08\n",
       " 2            DimitryRukhadze         ForkEvent  2022-06-02\n",
       " 3                   jursjohn         ForkEvent  2022-06-04\n",
       " 4                   MyStOrI1         ForkEvent  2022-06-09\n",
       " ...                      ...               ...         ...\n",
       " 77192845            UTkraken  PullRequestEvent  2022-06-12\n",
       " 77192846          NightWolfq  PullRequestEvent  2022-06-11\n",
       " 77192847  GulajavaMinistudio  PullRequestEvent  2022-06-11\n",
       " 77192848            umjammer  PullRequestEvent  2022-06-11\n",
       " 77192849              abhndv  PullRequestEvent  2022-06-11\n",
       " \n",
       " [77192850 rows x 3 columns],\n",
       "                    user_login        event_type  event_date\n",
       " 0                      etxxlq         ForkEvent  2022-07-06\n",
       " 1               Krishnamurtyp         ForkEvent  2022-07-05\n",
       " 2                    istoicov         ForkEvent  2022-07-05\n",
       " 3                      lynfin         ForkEvent  2022-07-05\n",
       " 4            bridgecrew-perf6         ForkEvent  2022-07-06\n",
       " ...                       ...               ...         ...\n",
       " 81544381              yuuulya  PullRequestEvent  2022-07-07\n",
       " 81544382      dependabot[bot]  PullRequestEvent  2022-07-04\n",
       " 81544383  github-actions[bot]  PullRequestEvent  2022-07-04\n",
       " 81544384      dependabot[bot]  PullRequestEvent  2022-07-05\n",
       " 81544385      dependabot[bot]  PullRequestEvent  2022-07-05\n",
       " \n",
       " [81544386 rows x 3 columns],\n",
       "                user_login        event_type  event_date\n",
       " 0                 jgibbes         ForkEvent  2022-08-04\n",
       " 1               wrwilliam         ForkEvent  2022-08-05\n",
       " 2                   JNN91         ForkEvent  2022-08-04\n",
       " 3             sammypijamy         ForkEvent  2022-08-04\n",
       " 4               Johnshall         ForkEvent  2022-08-05\n",
       " ...                   ...               ...         ...\n",
       " 86438276          hahnlee  PullRequestEvent  2022-08-07\n",
       " 86438277  dependabot[bot]  PullRequestEvent  2022-08-06\n",
       " 86438278            hasft  PullRequestEvent  2022-08-06\n",
       " 86438279   devilkiller-ag  PullRequestEvent  2022-08-06\n",
       " 86438280        pull[bot]  PullRequestEvent  2022-08-06\n",
       " \n",
       " [86438281 rows x 3 columns],\n",
       "                user_login        event_type  event_date\n",
       " 0            williamwzeng         ForkEvent  2022-09-05\n",
       " 1               DecentCr8         ForkEvent  2022-09-06\n",
       " 2          nikkisigurdson         ForkEvent  2022-09-12\n",
       " 3         analyticsworld1         ForkEvent  2022-09-12\n",
       " 4             Wenshiqi222         ForkEvent  2022-09-05\n",
       " ...                   ...               ...         ...\n",
       " 91432558          agateau  PullRequestEvent  2022-09-10\n",
       " 91432559         snyk-bot  PullRequestEvent  2022-09-10\n",
       " 91432560  dependabot[bot]  PullRequestEvent  2022-09-10\n",
       " 91432561      BrianLusina  PullRequestEvent  2022-09-18\n",
       " 91432562          Croniks  PullRequestEvent  2022-09-18\n",
       " \n",
       " [91432563 rows x 3 columns],\n",
       "                     user_login        event_type  event_date\n",
       " 0            appseed-projects2         PushEvent  2022-10-01\n",
       " 1                  elisa-panal         PushEvent  2022-10-01\n",
       " 2             himanshuramoliya  PullRequestEvent  2022-10-01\n",
       " 3          github-actions[bot]         PushEvent  2022-10-01\n",
       " 4                 zhengwschina         PushEvent  2022-10-01\n",
       " ...                        ...               ...         ...\n",
       " 102252921            Mihir0000  PullRequestEvent  2022-10-31\n",
       " 102252922        renovate[bot]         PushEvent  2022-10-31\n",
       " 102252923             tnozicka         PushEvent  2022-10-31\n",
       " 102252924  github-actions[bot]       CreateEvent  2022-10-31\n",
       " 102252925                xsidc         PushEvent  2022-10-31\n",
       " \n",
       " [102252926 rows x 3 columns],\n",
       "                 user_login        event_type  event_date\n",
       " 0                   Rater1         ForkEvent  2022-11-16\n",
       " 1                 kalra001         ForkEvent  2022-11-10\n",
       " 2                 thorn929         ForkEvent  2022-11-09\n",
       " 3                  MetaFam         ForkEvent  2022-11-23\n",
       " 4                damplex22         ForkEvent  2022-11-08\n",
       " ...                    ...               ...         ...\n",
       " 92842916   dependabot[bot]  PullRequestEvent  2022-11-20\n",
       " 92842917     renovate[bot]  PullRequestEvent  2022-11-20\n",
       " 92842918  sourcery-ai[bot]  PullRequestEvent  2022-11-20\n",
       " 92842919         pull[bot]  PullRequestEvent  2022-11-05\n",
       " 92842920        depfu[bot]  PullRequestEvent  2022-11-05\n",
       " \n",
       " [92842921 rows x 3 columns],\n",
       "                user_login        event_type  event_date\n",
       " 0          Peacebeuponu94         ForkEvent  2022-12-05\n",
       " 1             mdxprograms         ForkEvent  2022-12-05\n",
       " 2              tdrabikdev         ForkEvent  2022-12-05\n",
       " 3                eyalatox         ForkEvent  2022-12-06\n",
       " 4            nebulatechio         ForkEvent  2022-12-05\n",
       " ...                   ...               ...         ...\n",
       " 98136530           jnyfah  PullRequestEvent  2022-12-04\n",
       " 98136531  dependabot[bot]  PullRequestEvent  2022-12-04\n",
       " 98136532  dependabot[bot]  PullRequestEvent  2022-12-03\n",
       " 98136533          kenny-f  PullRequestEvent  2022-12-03\n",
       " 98136534  dependabot[bot]  PullRequestEvent  2022-12-03\n",
       " \n",
       " [98136535 rows x 3 columns],\n",
       "                    user_login   event_type  event_date\n",
       " 0               ThePersonThat    PushEvent  2023-01-01\n",
       " 1                renovate-bot    PushEvent  2023-01-01\n",
       " 2                   andypiper  CreateEvent  2023-01-01\n",
       " 3                    SrVertex    PushEvent  2023-01-01\n",
       " 4         github-actions[bot]    PushEvent  2023-01-01\n",
       " ...                       ...          ...         ...\n",
       " 97514944             zzollozz    PushEvent  2023-01-31\n",
       " 97514945              JDapiux    ForkEvent  2023-01-31\n",
       " 97514946            AlphaS14K  CreateEvent  2023-01-31\n",
       " 97514947           cjustin141    PushEvent  2023-01-31\n",
       " 97514948             19920513    PushEvent  2023-01-31\n",
       " \n",
       " [97514949 rows x 3 columns],\n",
       "                user_login        event_type  event_date\n",
       " 0             babushediao         ForkEvent  2023-02-18\n",
       " 1              lvannote17         ForkEvent  2023-02-27\n",
       " 2                nXmaniac         ForkEvent  2023-02-21\n",
       " 3          bjordaan-nmisa         ForkEvent  2023-02-21\n",
       " 4                    A91y         ForkEvent  2023-02-06\n",
       " ...                   ...               ...         ...\n",
       " 87328695   RafaelFerSilva  PullRequestEvent  2023-02-26\n",
       " 87328696         snyk-bot  PullRequestEvent  2023-02-18\n",
       " 87328697        pull[bot]  PullRequestEvent  2023-02-04\n",
       " 87328698        t-creates  PullRequestEvent  2023-02-04\n",
       " 87328699  dependabot[bot]  PullRequestEvent  2023-02-19\n",
       " \n",
       " [87328700 rows x 3 columns],\n",
       "                 user_login        event_type  event_date\n",
       " 0                  HMUdara         ForkEvent  2023-03-06\n",
       " 1         lars-amberscript         ForkEvent  2023-03-06\n",
       " 2                   gelkik         ForkEvent  2023-03-06\n",
       " 3                 hhy-hook         ForkEvent  2023-03-06\n",
       " 4             VincentCroft         ForkEvent  2023-03-05\n",
       " ...                    ...               ...         ...\n",
       " 96604646       AbhayShaw01  PullRequestEvent  2023-03-08\n",
       " 96604647         yinglejia  PullRequestEvent  2023-03-08\n",
       " 96604648   dependabot[bot]  PullRequestEvent  2023-03-07\n",
       " 96604649         pull[bot]  PullRequestEvent  2023-03-05\n",
       " 96604650           Toranks  PullRequestEvent  2023-03-05\n",
       " \n",
       " [96604651 rows x 3 columns],\n",
       "                  user_login        event_type  event_date\n",
       " 0              Daveyvdweide         ForkEvent  2023-04-03\n",
       " 1           wahahababaozhou         ForkEvent  2023-04-07\n",
       " 2                zmh2000829         ForkEvent  2023-04-15\n",
       " 3                  waynexia         ForkEvent  2023-04-15\n",
       " 4               taimurzahid         ForkEvent  2023-04-10\n",
       " ...                     ...               ...         ...\n",
       " 89556761            vs-2003  PullRequestEvent  2023-04-09\n",
       " 89556762        percivalalb  PullRequestEvent  2023-04-15\n",
       " 89556763          pull[bot]  PullRequestEvent  2023-04-08\n",
       " 89556764  nandiniachugatala  PullRequestEvent  2023-04-08\n",
       " 89556765            Yarondr  PullRequestEvent  2023-04-08\n",
       " \n",
       " [89556766 rows x 3 columns],\n",
       "                  user_login        event_type  event_date\n",
       " 0                Muqingluan         ForkEvent  2023-05-08\n",
       " 1              micah-bitech         ForkEvent  2023-05-06\n",
       " 2                Tokito-Kun         ForkEvent  2023-05-16\n",
       " 3         tanthanhtrinh-dev         ForkEvent  2023-05-22\n",
       " 4           rosemaryupdates         ForkEvent  2023-05-10\n",
       " ...                     ...               ...         ...\n",
       " 90315680             becky3  PullRequestEvent  2023-05-13\n",
       " 90315681          gorushkin  PullRequestEvent  2023-05-13\n",
       " 90315682   Darragh-Grealish  PullRequestEvent  2023-05-13\n",
       " 90315683        RohanMagar7  PullRequestEvent  2023-05-13\n",
       " 90315684   AhmedMohamedZein  PullRequestEvent  2023-05-13\n",
       " \n",
       " [90315685 rows x 3 columns],\n",
       "                 user_login        event_type  event_date\n",
       " 0         knighteartheater         ForkEvent  2023-06-13\n",
       " 1              Brat-Shcafa         ForkEvent  2023-06-07\n",
       " 2            GoSquidMaster         ForkEvent  2023-06-07\n",
       " 3                niybitgod         ForkEvent  2023-06-19\n",
       " 4                goroh2918         ForkEvent  2023-06-19\n",
       " ...                    ...               ...         ...\n",
       " 86520399         EvgenBatt  PullRequestEvent  2023-06-10\n",
       " 86520400       eoin-betdex  PullRequestEvent  2023-06-10\n",
       " 86520401           Jaumoso  PullRequestEvent  2023-06-10\n",
       " 86520402             EQuak  PullRequestEvent  2023-06-10\n",
       " 86520403      anuradhawick  PullRequestEvent  2023-06-10\n",
       " \n",
       " [86520404 rows x 3 columns]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monthly_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m number_of_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmonthly_results\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of DataFrames in monthly_results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber_of_items\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'monthly_results' is not defined"
     ]
    }
   ],
   "source": [
    "number_of_items = len(monthly_results)\n",
    "print(f\"Number of DataFrames in monthly_results: {number_of_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monthly_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Access the first month's DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m first_month_df \u001b[38;5;241m=\u001b[39m \u001b[43mmonthly_results\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check for duplicate user_login values\u001b[39;00m\n\u001b[0;32m      5\u001b[0m duplicates \u001b[38;5;241m=\u001b[39m first_month_df\u001b[38;5;241m.\u001b[39mduplicated(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_login\u001b[39m\u001b[38;5;124m'\u001b[39m, keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'monthly_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Access the first month's DataFrame\n",
    "first_month_df = monthly_results[0]\n",
    "\n",
    "# Check for duplicate user_login values\n",
    "duplicates = first_month_df.duplicated(subset='user_login', keep=False)\n",
    "\n",
    "# Print the number of duplicates found\n",
    "print(f\"Number of duplicates in the first month: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_month_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#month1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Group by 'user_login', aggregating event dates into lists and counting event types\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m aggregated_df0 \u001b[38;5;241m=\u001b[39m \u001b[43mfirst_month_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_login\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m      4\u001b[0m     event_dates\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mNamedAgg(column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_date\u001b[39m\u001b[38;5;124m'\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(x)),\n\u001b[0;32m      5\u001b[0m     event_types\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mNamedAgg(column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m'\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(x)),\n\u001b[0;32m      6\u001b[0m     event_counts\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mNamedAgg(column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m'\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display the DataFrame to verify the result\u001b[39;00m\n\u001b[0;32m     10\u001b[0m aggregated_df0\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'first_month_df' is not defined"
     ]
    }
   ],
   "source": [
    "#month1\n",
    "# Group by 'user_login', aggregating event dates into lists and counting event types\n",
    "aggregated_df0 = first_month_df.groupby('user_login').agg(\n",
    "    event_dates=pd.NamedAgg(column='event_date', aggfunc=lambda x: list(x)),\n",
    "    event_types=pd.NamedAgg(column='event_type', aggfunc=lambda x: list(x)),\n",
    "    event_counts=pd.NamedAgg(column='event_type', aggfunc='size')\n",
    ").reset_index()\n",
    "\n",
    "# Display the DataFrame to verify the result\n",
    "aggregated_df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in the first month: 78513577\n"
     ]
    }
   ],
   "source": [
    "# Access the first month's DataFrame\n",
    "second_month_df = monthly_results[1]\n",
    "\n",
    "# Check for duplicate user_login values\n",
    "duplicates = first_month_df.duplicated(subset='user_login', keep=False)\n",
    "\n",
    "# Print the number of duplicates found\n",
    "print(f\"Number of duplicates in the first month: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#month 2\n",
    "# Group by 'user_login', aggregating event dates into lists and counting event types\n",
    "aggregated_df = first_month_df.groupby('user_login').agg(\n",
    "    event_dates=pd.NamedAgg(column='event_date', aggfunc=lambda x: list(x)),\n",
    "    event_types=pd.NamedAgg(column='event_type', aggfunc=lambda x: list(x)),\n",
    "    event_counts=pd.NamedAgg(column='event_type', aggfunc='size')\n",
    ").reset_index()\n",
    "\n",
    "# Display the DataFrame to verify the result\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_login</th>\n",
       "      <th>event_dates</th>\n",
       "      <th>event_types</th>\n",
       "      <th>event_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0--key</td>\n",
       "      <td>[2022-03-22]</td>\n",
       "      <td>[PushEvent]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-000</td>\n",
       "      <td>[2022-03-06, 2022-03-06, 2022-03-06]</td>\n",
       "      <td>[PushEvent, ForkEvent, PushEvent]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-001-0</td>\n",
       "      <td>[2022-03-15]</td>\n",
       "      <td>[PushEvent]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-0Bruh</td>\n",
       "      <td>[2022-03-06, 2022-03-06]</td>\n",
       "      <td>[CreateEvent, CreateEvent]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-0devJSH</td>\n",
       "      <td>[2022-03-02, 2022-03-02, 2022-03-02, 2022-03-02]</td>\n",
       "      <td>[ForkEvent, ForkEvent, PushEvent, PushEvent]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_login                                       event_dates  \\\n",
       "0     0--key                                      [2022-03-22]   \n",
       "1      0-000              [2022-03-06, 2022-03-06, 2022-03-06]   \n",
       "2    0-001-0                                      [2022-03-15]   \n",
       "3    0-0Bruh                          [2022-03-06, 2022-03-06]   \n",
       "4  0-0devJSH  [2022-03-02, 2022-03-02, 2022-03-02, 2022-03-02]   \n",
       "\n",
       "                                    event_types  event_counts  \n",
       "0                                   [PushEvent]             1  \n",
       "1             [PushEvent, ForkEvent, PushEvent]             3  \n",
       "2                                   [PushEvent]             1  \n",
       "3                    [CreateEvent, CreateEvent]             2  \n",
       "4  [ForkEvent, ForkEvent, PushEvent, PushEvent]             4  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All user_login values are unique: True\n"
     ]
    }
   ],
   "source": [
    "def aggregate_monthly_data(df):\n",
    "    return df.groupby('user_login').agg(\n",
    "        event_dates=pd.NamedAgg(column='event_date', aggfunc=lambda x: list(x)),\n",
    "        event_types=pd.NamedAgg(column='event_type', aggfunc=lambda x: list(x)),\n",
    "        event_counts=pd.NamedAgg(column='event_type', aggfunc='size')\n",
    "    ).reset_index()\n",
    "\n",
    "aggregated_monthly_dfs = []\n",
    "\n",
    "for month_df in monthly_results:\n",
    "    aggregated_df = aggregate_monthly_data(month_df)\n",
    "    aggregated_monthly_dfs.append(aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_aggregated_df = pd.concat(aggregated_monthly_dfs, ignore_index=True)\n",
    "\n",
    "final_aggregated_df.to_csv('path/to/your/final_aggregated_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in month 1: 80334006\n",
      "Number of duplicates in month 2: 78513577\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 606. MiB for an array with shape (79453861,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of duplicates in month \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicates\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Aggregate the current month's DataFrame\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m aggregated_df \u001b[38;5;241m=\u001b[39m \u001b[43mmonth_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_login\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNamedAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNamedAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNamedAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Append the aggregated DataFrame to the list\u001b[39;00m\n\u001b[0;32m     17\u001b[0m aggregated_monthly_dfs\u001b[38;5;241m.\u001b[39mappend(aggregated_df)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\groupby\\generic.py:979\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[0;32m    978\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args, kwargs)\n\u001b[1;32m--> 979\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:161\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(arg):\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:435\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    432\u001b[0m     results \u001b[38;5;241m=\u001b[39m {key: colg\u001b[38;5;241m.\u001b[39magg(how) \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m--> 435\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    436\u001b[0m         key: obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39magg(how) \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    437\u001b[0m     }\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m# set the final keys\u001b[39;00m\n\u001b[0;32m    440\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(arg\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:436\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m     results \u001b[38;5;241m=\u001b[39m {key: colg\u001b[38;5;241m.\u001b[39magg(how) \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m    435\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 436\u001b[0m         key: \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gotitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    437\u001b[0m     }\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m# set the final keys\u001b[39;00m\n\u001b[0;32m    440\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(arg\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\groupby\\generic.py:249\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, abc\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;66;03m# Catch instances of lists / tuples\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[1;32m--> 249\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_multiple_funcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;66;03m# \"Optional[List[str]]\", variable has type \"Index\")\u001b[39;00m\n\u001b[0;32m    253\u001b[0m         ret\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m columns  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\groupby\\generic.py:303\u001b[0m, in \u001b[0;36mSeriesGroupBy._aggregate_multiple_funcs\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, func) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arg):\n\u001b[0;32m    302\u001b[0m     key \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mOutputKey(label\u001b[38;5;241m=\u001b[39mname, position\u001b[38;5;241m=\u001b[39midx)\n\u001b[1;32m--> 303\u001b[0m     results[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, DataFrame) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\groupby\\generic.py:265\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# TODO: KeyError is raised in _python_agg_general,\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m#  see test_groupby.test_basic\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_named(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\groupby\\groupby.py:1332\u001b[0m, in \u001b[0;36mGroupBy._python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1328\u001b[0m name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# if this function is invalid for this dtype, we will ignore it.\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1334\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropping invalid columns in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.agg \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis deprecated. In a future version, a TypeError will be raised. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   1341\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\groupby\\ops.py:1049\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m   1046\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_series_pure_python(obj, func)\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1051\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\groupby\\ops.py:1072\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_fast\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m   1070\u001b[0m indexer \u001b[38;5;241m=\u001b[39m get_group_index_sorter(ids, ngroups)\n\u001b[0;32m   1071\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m-> 1072\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m sgrouper \u001b[38;5;241m=\u001b[39m libreduction\u001b[38;5;241m.\u001b[39mSeriesGrouper(obj, func, ids, ngroups)\n\u001b[0;32m   1074\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m sgrouper\u001b[38;5;241m.\u001b[39mget_result()\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 606. MiB for an array with shape (79453861,) and data type int64"
     ]
    }
   ],
   "source": [
    "aggregated_monthly_dfs = []  # Initialize an empty list to store aggregated DataFrames\n",
    "\n",
    "# Iterate through each month's DataFrame in monthly_results\n",
    "for i, month_df in enumerate(monthly_results):\n",
    "    # Check for duplicate user_login values in the current month's DataFrame\n",
    "    duplicates = month_df.duplicated(subset='user_login', keep=False)\n",
    "    print(f\"Number of duplicates in month {i+1}: {duplicates.sum()}\")\n",
    "    \n",
    "    # Aggregate the current month's DataFrame\n",
    "    aggregated_df = month_df.groupby('user_login').agg(\n",
    "        event_dates=pd.NamedAgg(column='event_date', aggfunc=lambda x: list(x)),\n",
    "        event_types=pd.NamedAgg(column='event_type', aggfunc=lambda x: list(x)),\n",
    "        event_counts=pd.NamedAgg(column='event_type', aggfunc='size')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Append the aggregated DataFrame to the list\n",
    "    aggregated_monthly_dfs.append(aggregated_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
